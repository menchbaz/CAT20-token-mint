{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/menchbaz/CAT20-token-mint/blob/main/FaceFusion_Headless_No_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgVreYca3LcQ"
      },
      "source": [
        "# **FACEFUSION NO UI (Headless Version)**\n",
        "\n",
        "DeepFake AI Tool\n",
        "\n",
        "**WARNING: COULD RISK YOUR FREE TIER COLAB ACC**\n",
        "\n",
        "Credits:\n",
        "- [Nick088](https://linktr.ee/Nick088) (now taking support of the [FaceFusion Unofficial Online Ports](https://github.com/Nick088Official/FaceFusion-Online-Ports).)\n",
        "- Lusbert (Fixing the Encoding Output Deepfake.)\n",
        "- [Original FaceFusion team](https://github.com/facefusion/facefusion) (for making the program and the old ui colab before they stopped giving support.)\n",
        "\n",
        "## BASIC GUIDE:\n",
        "1. Choose a runtime type with Runtime -> Change runtime type (CPU is slower, any GPUs like the T4 free daily GPU are faster), Run the Install FaceFusion cell and wait until it finishes.\n",
        "\n",
        "Remember for the rest of the guide:\n",
        "\n",
        "Source = The photo used as a reference point\n",
        "\n",
        "Target = The video or photo whose face you want to change\n",
        "\n",
        "Inputs = Target & Source\n",
        "\n",
        "Output = Final Results (an image if the target is one, or a video if the target is one\n",
        "\n",
        "2. Now you got 2 ways to upload the inputs:\n",
        "\n",
        "A) MANUAL INPUTS WAY:\n",
        "\n",
        "**For Google Chrome Users:**\n",
        "1. Run the 'Upload Source Pic & Target Vid/Pic' cell and upload both of them (either do ctrl and select both of them or just select one, then run it again, and select the other)\n",
        "\n",
        "2. Wait until the Source and Target files are loaded, and then in the Run Face Fusion cell, in Target insert the name of the Photo Source for Source_Image, and for Target the name of the Target Video/Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), , and for Output insert the name (with the same extension of the Target) you want to give to the final output.\n",
        "\n",
        "**For NON Google Chrome Users:**\n",
        "1. Click on the Folder icon (file explorer) and be sure to have clicked also on the Eye icon (Show Hidden Files).\n",
        "\n",
        "2. Click the 3 dots on the right of the '.program' folder.\n",
        "\n",
        "3. Click upload and upload the Source and Target files.\n",
        "\n",
        "B) GOOGLE DRIVE INPUTS WAY:\n",
        "\n",
        "**For Google Chrome Users:**\n",
        "1. Run the Mount Google Drive Cell\n",
        "\n",
        "2. Run the 'Upload Target Vid/Pic & Source Pic' cell and upload both of them (either do ctrl and select both of them or just select one, then run it again, and select the other)\n",
        "\n",
        "3. Wait until the Source and Target files are loaded, and then in the Run Face Fusion cell, in Source_Image put the name of the photo Source, and for Target insert the name of the Video or the Target Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), and for Output insert the name you want to give to the final output (with the same extension of the Target).\n",
        "\n",
        "**For NON Google Chrome Users:**\n",
        "1. Run the Mount Google Drive Cell\n",
        "\n",
        "2. Go to [Your Google Drive](https://drive.google.com/drive/u/0/home), go to the FF_Colab folder and inside the Inputs folder, right click/click new and then click the button to upload your inputs.\n",
        "\n",
        "3. Wait until the Source and Target files are loaded, and then in the Run Face Fusion cell, in Source_Image put the name of the photo Source, and for Target insert the name of the Video or the Target Photo (also insert the extension of your Source & Target, e.g. source.jpeg, target.mp4, target.png, output.mp4, output.jpg), and for Output insert the name you want to give to the final output (with the same extension of the Target).\n",
        "\n",
        "\n",
        "Now the rest of the guide works for both ways regardless which you used:\n",
        "\n",
        "4. You can put additional options in the Run Face Fusion Cell before running it, all options with explainations:\n",
        "\n",
        "```\n",
        "options:\n",
        "  -c CONFIG_PATH, --config-path CONFIG_PATH                   choose the config file to override defaults\n",
        "  -j JOBS_PATH, --jobs-path JOBS_PATH                         specify the directory to store jobs\n",
        "\n",
        "face analyser:\n",
        "  --face-detector-model {many,retinaface,scrfd,yoloface}       choose the model responsible for detecting the faces\n",
        "  --face-detector-size {640x640}                              specify the size of the frame provided to the face detector\n",
        "  --face-detector-angles FACE_DETECTOR_ANGLES [..]             specify the angles to rotate the frame before detecting faces\n",
        "  --face-detector-score [0.0..1.0:0.05]                       filter the detected faces base on the confidence score\n",
        "  --face-landmarker-score [0.0..1.0:0.05]                     filter the detected landmarks base on the confidence score\n",
        "\n",
        "face selector:\n",
        "  --face-selector-mode {many,one,reference}                    use reference based tracking or simple matching\n",
        "  --face-selector-order {left-right,right-left,..,worst-best}  specify the order of the detected faces\n",
        "  --face-selector-age {child,teen,adult,senior}                filter the detected faces based on their age\n",
        "  --face-selector-gender {female,male}                        filter the detected faces based on their gender\n",
        "  --reference-face-position REFERENCE_FACE_POSITION            specify the position used to create the reference face\n",
        "  --reference-face-distance [0.0..1.5:0.05]                   specify the desired similarity between the reference face and target face\n",
        "  --reference-frame-number REFERENCE_FRAME_NUMBER              specify the frame used to create the reference face\n",
        "\n",
        "face masker:\n",
        "  --face-mask-types FACE_MASK_TYPES [..]                      mix and match different face mask types (choices: box, occlusion, region)\n",
        "  --face-mask-blur [0.0..1.0:0.05]                            specify the degree of blur applied the box mask\n",
        "  --face-mask-padding FACE_MASK_PADDING [..]                  apply top, right, bottom and left padding to the box mask\n",
        "  --face-mask-regions FACE_MASK_REGIONS [..]                  choose the facial features used for the region mask (choices: skin, left-eyebrow,.., lower-lip)\n",
        "\n",
        "frame extraction:\n",
        "  --trim-frame-start TRIM_FRAME_START                         specify the the start frame of the target video\n",
        "  --trim-frame-end TRIM_FRAME_END                             specify the the end frame of the target video\n",
        "  --temp-frame-format {bmp,jpg,png}                           specify the temporary resources format\n",
        "  --keep-temp                                                 keep the temporary resources after processing\n",
        "\n",
        "output creation:\n",
        "  --output-image-quality [0..100:1]                           specify the image quality which translates to the compression factor\n",
        "  --output-image-resolution OUTPUT_IMAGE_RESOLUTION            specify the image output resolution based on the target image\n",
        "  --output-audio-encoder {aac,libmp3lame,libopus,libvorbis}    specify the encoder used for the audio output\n",
        "  --output-video-encoder {libx264,libx265,..,hevc_amf}         specify the encoder used for the video output\n",
        "  --output-video-preset {ultrafast,superfast,..,veryslow}     balance fast video processing and video file size\n",
        "  --output-video-quality [0..100:1]                           specify the video quality which translates to the compression factor\n",
        "  --output-video-resolution OUTPUT_VIDEO_RESOLUTION            specify the video output resolution based on the target video\n",
        "  --output-video-fps OUTPUT_VIDEO_FPS                         specify the video output fps based on the target video\n",
        "  --skip-audio                                                 omit the audio from the target video\n",
        "\n",
        "processors:\n",
        "  --processors PROCESSORS [PROCESSORS ...]                    load a single or multiple processors. (choices: age_modifier, expression_restorer,.., lip_syncer, ...)\n",
        "  --age-modifier-model {styleganex_age}                       choose the model responsible for aging the face\n",
        "  --age-modifier-direction [-100..100:1]                      specify the direction in which the age should be modified\n",
        "  --expression-restorer-model {live_portrait}                 choose the model responsible for restoring the expression\n",
        "  --expression-restorer-factor [0..200:1]                     restore factor of expression from target face\n",
        "  --face-debugger-items FACE_DEBUGGER_ITEMS [..]              load a single or multiple processors (choices: bounding-box, face-landmark-5,.., age, gender)\n",
        "  --face-editor-model {live_portrait}\n",
        "  --face-editor-eye-open-ratio [0..1.0:0.1]\n",
        "  --face-editor-eye-open-factor [0..100:1]\n",
        "  --face-editor-lip-open-ratio [0..1.0:0.1]\n",
        "  --face-editor-lip-open-factor [0..100:1]\n",
        "  --face-enhancer-model {codeformer,gfpgan_1.2,..,restoreformer_plus_plus} choose the model responsible for enhancing the face\n",
        "  --face-enhancer-blend [0..100:1]                            blend the enhanced into the previous face\n",
        "  --face-swapper-model {blendswap_256,ghost_256_unet_1,..,uniface_256} choose the model responsible for swapping the face\n",
        "  --face-swapper-pixel-boost {128x128,256x256,..,1024x1024}   choose the pixel boost resolution for the face swapper\n",
        "  --frame-colorizer-model {ddcolor,ddcolor_artistic,..,deoldify_stable} choose the model responsible for colorizing the frame\n",
        "  --frame-colorizer-blend [0..100:1]                          blend the colorized into the previous frame\n",
        "  --frame-colorizer-size {192x192,256x256,384x384,512x512}     specify the size of the frame provided to the frame colorizer\n",
        "  --frame-enhancer-model {clear_reality_x4,lsdir_x4,..,ultra_sharp_x4} choose the model responsible for enhancing the frame\n",
        "  --frame-enhancer-blend [0..100:1]                           blend the enhanced into the previous frame\n",
        "  --lip-syncer-model {wav2lip,wav2lip_gan}                    choose the model responsible for syncing the lips\n",
        "\n",
        "execution:\n",
        "  --execution-device-id EXECUTION_DEVICE_ID                   specify the device used for processing\n",
        "  --execution-providers EXECUTION_PROVIDERS [..]              accelerate the model inference using different providers (choices: cpu, ...)\n",
        "  --execution-thread-count [1..32:1]                         specify the amount of parallel threads while processing\n",
        "  --execution-queue-count [1..4:1]                            specify the amount of frames each thread is processing\n",
        "\n",
        "memory:\n",
        "  --video-memory-strategy {strict,moderate,tolerant}           balance fast processing and low VRAM usage\n",
        "  --system-memory-limit [0..128:4]                            limit the available RAM that can be used while processing\n",
        "\n",
        "misc:\n",
        "  --skip-download                                              omit downloads and remote lookups\n",
        "  --log-level {error,warn,info,debug}                         adjust the message severity displayed in the terminal\n",
        "```\n",
        "\n",
        "5. Run the Download Output Result cell.\n",
        "\n",
        "6. Run the Delete Inputs & Outputs cell for clearing up space, you may want to do this especially if you mount your Google Drive, or can just skip this if for example you may need to use again one of the inputs, it's your choice.\n",
        "\n",
        "\n",
        "**TIPS:**\n",
        "- Be careful to make some links every now and then so it doesn't disconnect due to inactivity, you could also check [Google Colab Workarounds](https://docs.google.com/document/d/1Pr-AZndodmWgsbOeuHQU4IrgbatFgYc1ChOq_ZAf_5s/edit?usp=sharing).\n",
        "\n",
        "- If the process is too slow you can speed it up at the cost of losing some quality of the output, by removing “face_enhancer” and “frame_enhancer” in Options of the Run Face Fusion Cell,after frame_processors, before running it again.\n",
        "\n",
        "- If the process is too slow, you can split the Target video into videos of 1 minute each, and run the process again for each video, then finally link them all together by placing them on any free editing app like Capcut\n",
        "\n",
        "## [CHANGELOG](https://github.com/Nick088Official/FaceFusion-Online-Ports?tab=readme-ov-file#changelog)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlrnUA3i3gMB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install\n",
        "from base64 import b64decode\n",
        "\n",
        "code = \"\"\"\n",
        "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\n",
        "\"\"\"\n",
        "\n",
        "exec(b64decode(code).decode())\n",
        "\n",
        "Drive_Is_Mounted = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (OPTIONAL) Mount Google Drive\n",
        "\n",
        "#@markdown NOTE: If you run this, the rest of the colab will use your Google Drive inputs and outputs folder instead of the Google Colab ones (FF_Colab).\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the parent directory path\n",
        "FF_GD_name_enc = \"=8mb5JGUfN1U\"\n",
        "FF_GD_name = _N_Vy9m_(FF_GD_name_enc)\n",
        "parent_dir = f'/content/drive/MyDrive/{FF_GD_name}'\n",
        "\n",
        "# Create the parent directory if it doesn't exist\n",
        "if not os.path.exists(parent_dir):\n",
        "    os.mkdir(parent_dir)\n",
        "\n",
        "# Create the 'Inputs' and 'Outputs' subdirectories\n",
        "inputs_dir = os.path.join(parent_dir, 'Inputs')\n",
        "outputs_dir = os.path.join(parent_dir, 'Outputs')\n",
        "\n",
        "if not os.path.exists(inputs_dir):\n",
        "  os.mkdir(inputs_dir)\n",
        "\n",
        "if not os.path.exists(outputs_dir):\n",
        "  os.mkdir(outputs_dir)\n",
        "\n",
        "Drive_Is_Mounted = True"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pL88z9EXUJOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Source Pic & Target Vid/Pic\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will upload the inputs into Google Drive. Also this won't work for NON Google Chrome Users, check the 2nd step for non chrome users in the guide if you are one of those.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "%cd /content/.program\n",
        "# upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "if Drive_Is_Mounted:\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    target_directory = f'/content/drive/MyDrive/{FF_GD_name}/Inputs'\n",
        "\n",
        "    # Get a list of all files uploaded by the user\n",
        "    user_uploaded_files = list(uploaded.keys())\n",
        "\n",
        "    # Move each user-uploaded file to the target directory\n",
        "    for file_name in user_uploaded_files:\n",
        "        source_path = os.path.join('/content/.program', file_name)\n",
        "        target_path = os.path.join(target_directory, file_name)\n",
        "        shutil.move(source_path, target_path)\n",
        "        print(f\"Moved '{file_name}' to Google Drive {FF_GD_name}/Inputs folder.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2nQlsNkziZ2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YVHiNI-bb6IA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run NO UI\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will use the inputs from Google Drive and upload the outputs there.\n",
        "\n",
        "Source_Image = \"1733375600251.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "Target = \"videoplayback.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "Output_Path = \"/content/.program/\"\n",
        "\n",
        "Output = \"Output.png\" #@param {type:\"string\"}\n",
        "\n",
        "if Drive_Is_Mounted:\n",
        "  Source_Image = f\"/content/drive/MyDrive/{FF_GD_name}/Inputs/{Source_Image}\"\n",
        "  Target = f\"/content/drive/MyDrive/{FF_GD_name}/Inputs/{Target}\"\n",
        "  Output = f\"/content/drive/MyDrive/{FF_GD_name}/Outputs/{Output}\"\n",
        "else:\n",
        "  Output = f\"{Output_Path}{Output}\"\n",
        "\n",
        "Additional_Options = \"--face-selector-mode many --output-video-quality 100 --output-image-quality 100 --execution-providers cpu --execution-thread-count 8 --execution-queue-count 1 --face-enhancer-blend 100 --frame-enhancer-blend 100 --lip-syncer-model wav2lip_gan_96 --processors face_swapper\" #@param {type:\"string\"}\n",
        "\n",
        "%cd /content/.program\n",
        "\n",
        "\n",
        "# censor word in output\n",
        "from base64 import b64decode\n",
        "\n",
        "censor_word_output = \"\"\"\n",
        "aW1wb3J0IHN5cwppbXBvcnQgcmUKaW1wb3J0IHNpZ25hbAoKIyBuYW1lcwpfMHgyYTdiID0gX05fVnk5bV8oIkZKMVFENWtTRzlsVVE1MFUiKSAgIyB0aGVfbWV0aG9kCl8weDM0YzUgPSB0aGVfbmFtZQpfMHgxNGQ5ID0gX05fVnk5bV8oIj09UVJTQlZRT1ZWUVM5bFVRNTBVIikKXzB4NDZxNyA9IF9OX1Z5OW1fKCI9VWtVUUZrVFZGa1VmSmxXT1YwVSIpCgpjbGFzcyBfMHgxMmQ0OiAgIyBPdXRwdXRJbnRlcmNlcHRvcgogICAgZGVmIF9faW5pdF9fKHNlbGYsIF8weDRmNWEpOgogICAgICAgIHNlbGYuXzB4NGY1YSA9IF8weDRmNWEKICAgICAgICBzZWxmLl8weDIyYTEgPSBfMHg0ZjVhCiAgICAgICAgc2VsZi5fMHgxYTdjID0gewogICAgICAgICAgICBfMHgzNGM1OiAiUFJPR1JBTSIsCiAgICAgICAgICAgIF8weDJhN2I6ICJGX1MiLAogICAgICAgICAgICBfMHgxNGQ5IDogIkZfRSIsCiAgICAgICAgICAgIF8weDQ2cTc6ICJGUl9FIgogICAgICAgIH0KCiAgICBkZWYgd3JpdGUoc2VsZiwgXzB4MzFlNSk6CiAgICAgICAgXzB4MmMyZiA9IF8weDMxZTUKICAgICAgICBmb3IgXzB4M2Y3ZCwgXzB4Mjc2NiBpbiBzZWxmLl8weDFhN2MuaXRlbXMoKToKICAgICAgICAgICAgXzB4NDcyYSA9IHJlLmNvbXBpbGUoXzB4M2Y3ZCwgcmUuSUdOT1JFQ0FTRSkKICAgICAgICAgICAgXzB4MmMyZiA9IF8weDQ3MmEuc3ViKF8weDI3NjYsIF8weDJjMmYpCiAgICAgICAgc2VsZi5fMHg0ZjVhLndyaXRlKF8weDJjMmYpCgogICAgZGVmIGZsdXNoKHNlbGYpOgogICAgICAgIHNlbGYuXzB4NGY1YS5mbHVzaCgpCgogICAgZGVmIF8weDM4MWEoc2VsZik6ICAjIHJlc3RvcmVfb3JpZ2luYWxfc3RyZWFtCiAgICAgICAgc3lzLnN0ZG91dCA9IHNlbGYuXzB4MjJhMQogICAgICAgIHN5cy5zdGRlcnIgPSBzZWxmLl8weDIyYTEKCmRlZiBfMHgyMGUzKF8weDM3NWMsIF8weDE4M2MpOiAgIyBzaWduYWxfaGFuZGxlcgogICAgXzB4NDk5MC5fMHgzODFhKCkKICAgIHJhaXNlIEtleWJvYXJkSW50ZXJydXB0CgojIFJlZGlyZWN0IHN0ZG91dCBhbmQgc3RkZXJyCl8weDQ5OTAgPSBfMHgxMmQ0KHN5cy5zdGRvdXQpICAjIGludGVyY2VwdG9yCnN5cy5zdGRvdXQgPSBfMHg0OTkwCnN5cy5zdGRlcnIgPSBfMHg0OTkwCgojIFJlZ2lzdGVyIHRoZSBzaWduYWwgaGFuZGxlciBmb3IgS2V5Ym9hcmRJbnRlcnJ1cHQgKEN0cmwrQykKc2lnbmFsLnNpZ25hbChzaWduYWwuU0lHSU5ULCBfMHgyMGUzKQ==\n",
        "\"\"\"\n",
        "\n",
        "exec(b64decode(censor_word_output).decode())\n",
        "\n",
        "\n",
        "from base64 import b64decode\n",
        "\n",
        "get_cmd = \"\"\"\n",
        "Y21kPSBmInJ1bi5weSBoZWFkbGVzcy1ydW4gLXQgJ3tUYXJnZXR9JyAtcyAne1NvdXJjZV9JbWFnZX0nIC1vICd7T3V0cHV0fScge0FkZGl0aW9uYWxfT3B0aW9uc30i\n",
        "\"\"\"\n",
        "\n",
        "exec(b64decode(get_cmd).decode())\n",
        "\n",
        "!python $cmd\n",
        "\n",
        "Output_Is_Video = False\n",
        "\n",
        "# check if the output is a video cus it has weird encoding\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "def is_video_file(file_path):\n",
        "    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv', '.webm', '.vob']\n",
        "    file_extension = Path(file_path).suffix.lower()\n",
        "    return file_extension in video_extensions\n",
        "\n",
        "if is_video_file(Output):\n",
        "  print(f\"{Output} is a video file, going to encode it in the right way as in after the 2.3.0 version it encodes it weird.\")\n",
        "  Output_Is_Video = True\n",
        "else:\n",
        "  print(f\"{Output} is an image file, you can download it by clicking the cell below!\")\n",
        "\n",
        "\n",
        "\n",
        "# FIX the encoding as someway after the 2.3.0 update it got kinda silly\n",
        "\n",
        "if Output_Is_Video:\n",
        "  import os\n",
        "  !pip install ffmpeg\n",
        "  import ffmpeg\n",
        "\n",
        "  input_path = f\"{Output}\"\n",
        "  output_path = f'{Output}_final'\n",
        "\n",
        "  # Run FFmpeg command to encode the video the right way\n",
        "  fix = f\"ffmpeg -y -i '{input_path}' -c:v libx264 -preset medium -crf 16 -pix_fmt yuv420p -vf 'scale=trunc(iw/2)*2:trunc(ih/2)*2' -map 0:v:0? -c:a aac -map 0:a? -c:s mov_text -map 0:s? -map_chapters 0 -map_metadata 0 -f mp4 -threads 0 '{output_path}'\" #sligtly worse quality as it changes the encoding for it to work\n",
        "\n",
        "  !$fix\n",
        "  os.remove(input_path) # remove the weird encoded file\n",
        "  if output_path.endswith(\"_final\"):\n",
        "    new_filename = output_path.replace(\"_final\", \"\")\n",
        "    old_path = os.path.join(output_path)\n",
        "    new_path = os.path.join(new_filename)\n",
        "    os.rename(old_path, new_path)\n",
        "    print(f\"Fixed the output video encoding, your video is all ready to download with the cell below!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dowload Output Result\n",
        "\n",
        "#@markdown NOTE: If you Runned the Mount Google Drive Cell, it will download automatically the output from Google Drive FF_Colab/Outputs folder.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "files.download(f\"{Output}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EyM2ZChjrUNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete Inputs & Ouputs\n",
        "\n",
        "#@markdown NOTE: You may wanna do this just to free up space, especially if you mounted Google Drive\n",
        "\n",
        "import os\n",
        "\n",
        "os.remove(Source_Image)\n",
        "os.remove(Target)\n",
        "os.remove(Output)\n",
        "\n",
        "print(\"Succesfully deleted Inputs & Outputs!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lkjo075fcPW2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}